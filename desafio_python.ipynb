{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "desafio_python.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVKf/jKkkmFrTTjAhbpoDX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthHugh/desafio_python/blob/master/desafio_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL0anJ6o6Z-h"
      },
      "source": [
        "# Preparo do ambiente\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_cKPE7U2V0q"
      },
      "source": [
        "!git clone https://github.com/projeto22/challenge-python.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RskiYLV5zRl6"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1DdM1U4z6R7"
      },
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFoSBj4d0GHp"
      },
      "source": [
        "# iniciar uma sessão local \n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxQvgQTI6jmh"
      },
      "source": [
        "# Dicionário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNcd1pr7cjEL"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "\n",
        "\n",
        "files = [f for f in os.listdir('challenge-python/dataset')]\n",
        "\n",
        "lista_final = []\n",
        "\n",
        "\n",
        "for filename in files:\n",
        "  with open('challenge-python/dataset/'+filename, 'r', encoding='cp1252') as f:\n",
        "    # Lê - tudo minuscula - separa palavras numa lista\n",
        "    words = f.read().lower().split()  \n",
        "\n",
        "    # tira plurais e caracteres especiais, de forma que o dicionário armazene apenas as palavras\n",
        "    words = [word.replace(\"'s\", '') for word in words]\n",
        "    words = [re.sub('\\W+','', word ) for word in words]\n",
        "\n",
        "    #à cada iteração Junta lista por lista de palavras\n",
        "    lista_final.extend(words) \n",
        "\n",
        "#dataframe com todas as palavras distintas\n",
        "df_palavras = sc.createDataFrame(lista_final, StringType()).distinct()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-XykaYC6vm4"
      },
      "source": [
        "# Adiciona o código de cada palavra\n",
        "rdd_df = df_palavras.rdd.zipWithIndex()\n",
        "df_final = rdd_df.toDF()\n",
        "\n",
        "df_final = df_final.withColumn('word', df_final['_1'].getItem(\"value\"))\n",
        "df_final = df_final.withColumnRenamed('_2','id_word')\n",
        "\n",
        "# Monta a estrutura do dicionário\n",
        "df_dicionario = df_final.select('word','id_word')\n",
        "# Roube e adicionei cabeçalho ao arquivo salvo para facilitar a leitura no próximo Job :p sei nem se poderia hehehe\n",
        "df_dicionario.write.mode('overwrite').option(\"sep\",\"\\t\").option(\"header\", \"true\").csv('challenge-python/dicionario')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhGGzggsf6CF"
      },
      "source": [
        "# INDICIE REVERSO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEduZAg4De1_"
      },
      "source": [
        "import re\n",
        "from pyspark.sql.types import StructType,StructField, IntegerType\n",
        "import pyspark.sql.functions as sql\n",
        "\n",
        "df_recover_dic = sc.read.csv(\"challenge-python/dicionario\", inferSchema=True, header=True, sep='\\t')\n",
        "df_recover_dic = df_recover_dic.withColumn('id_word', df_recover_dic.id_word.cast('int'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnAXLAp3Gvvz"
      },
      "source": [
        "# Cria dataframe vazio, com dois campos\n",
        "schema = StructType([\n",
        "  StructField('id_word', IntegerType()),\n",
        "  StructField('id_file', IntegerType()),\n",
        "  ])\n",
        "df_pares_final = sc.createDataFrame(sc.sparkContext.emptyRDD(),schema)\n",
        "\n",
        "'''\n",
        "Para cada arquivo - separa todas as palavras em uma lista, faz join com dicionário para recuperar os IDs \n",
        "(considerando que não tenho acesso ao script de construção do dicionário)\n",
        "Monta os pares de código de palavras e código do arquivo\n",
        "Incrementa no dataframe criado anteriormente '''\n",
        "for filename in files:\n",
        "  with open('challenge-python/dataset/'+filename, 'r', encoding='cp1252') as f:\n",
        "    words = f.read().lower().split()  \n",
        "    words = [word.replace(\"'s\", '') for word in words]\n",
        "    words = [re.sub('\\W+','', word ) for word in words]\n",
        "    pares = []\n",
        "\n",
        "    df_pares = sc.createDataFrame(words, StringType())\n",
        "\n",
        "    filename = int(filename)\n",
        "    df_join = df_pares.join(df_recover_dic, df_pares.value == df_recover_dic.word, how='left')\n",
        "    df_join = df_join.withColumn('id_file', sql.lit(filename)).select('id_word','id_file')\n",
        "\n",
        "    df_pares_final = df_pares_final.union(df_join)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjVfHpoozN6L"
      },
      "source": [
        "# Tira os distintos, ordena e agrupa listas de arquivos por código de palavra\n",
        "df_pares_final = df_pares_final.distinct().orderBy('id_word','id_file')\n",
        "df_pares_final = df_pares_final.filter(df_pares_final.id_word.isNotNull())\n",
        "df_pares_final = df_pares_final.groupby('id_word').agg(sql.collect_list('id_file').alias('files')).orderBy('id_word')\n",
        "\n",
        "df_pares_final.write.mode('overwrite').option(\"sep\",\"\\t\").option(\"header\", \"true\").parquet('challenge-python/indice_reverso')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}